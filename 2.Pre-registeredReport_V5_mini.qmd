---
title: "Pre-registered report: Space Sequence Synesthesia Diagnostic using form mapping"
format:
  html:
    code-fold: true
execute:
  warning: false
  message: false
date: today
editor_options:
  markdown:
    wrap: 72
number-sections: true
link-citations: true
bibliography: references.bib
---

```{r}
#| eval: false
#| include: false

output:
  html_document: default
  
format:
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
```

```{r, libraries, message=FALSE, warning=FALSE}
# Data import
library(readr)
library(readxl)
# Data wrangling:
library(tidyr)
library(dplyr)
# Render Formatting
library(papaja)
library(RColorBrewer)
library(kableExtra)
# Plots
library(ggplot2)
library(ggridges)
library(ggalluvial)
# geometry/geography feature package
library(sf)
# for ROC analyses:
library(pROC) # See https://www.r-bloggers.com/2019/02/some-r-packages-for-roc-curves/
```

# Abstract:

Space sequence synesthesia is the phenomenon of representing ordered
visual symbols in particular spatial positions. Not everyone has space
sequence synesthesia. Existent tools to detect space sequence synethesia
are based on self-reports (i.e. questionnaire) and response consistency.
Consistency can be derived on the stimulus level, i.e. spatial deviation
in time, or on the form level, i.e. geometrical properties. In this
pre-registered report, available data from 467 participants is used to
explore new tools to discriminate syntheses from controls. Conceptually,
the novel criteria aim at taking advantage of the inducer's ordinality
that create synesthetic forms. For this aim, we harness a geography
package to extract geometrical features to use as a test for synethesia.
Reciever Operator Characteristics are used to select the features that
best classify the groups. In a second phase, we will test the predictive
power of the new diagnostic features on an additional dataset that has
yet to be collected.

# Introduction

Sequence Space Synesthesia (SSS) is defined as representing ordered
sequences in particular spatial positions. For example, numbers (the
*inducers*) are perceived as arranged particular positions in space (the
*concurrent*), also called number forms [@galton1880]. In addition to
numbers, SSS might also perceive weekdays and months in particular
arrangement in space.

Estimated prevalence for SSS in the general population spans between 4.4
% [@brang2013] and 14.2 % [@seron1992], see also [@ward2018;
@sagiv2006]. Hence a reliable diagnostic tool to detect SSS would also
be useful to investigate SSS. SSS forms are idiosyncratic, meaning they
each individual might have a different form. Idiosyncrasy makes it
difficult to diagnose SSS.

Diagnostic depends on the definition of the conditions under
investigation. A strict definition of Synestheesia requires five
different criteria [@deroy2013]. *Automaticity*: the *inducer*
automatically triggers the *concurrent*. For example 10 might
automatically be the number in the highest position.
*Unidirectionality*: while the *inducer* triggers the concurrent, the
concurrent does not trigger the inducer. Hence the highest position does
not trigger the number 10. *Consciousness*: The concurrent is
consciously perceived. *Developmentally early*: the experience was
already present during childhood. *Consistency*: the inducer-concurrent
pair remains stable in time. For example, the particular position
triggered by 10 is stable in time. Consistency is a well suited
charachteristic of synesthesia to develop a diagnostic tool. For esample
a consistency test is used to diagnose grapheme-colour synesthesia
[@rothen2013a]. While the original groups of synesthetes are mostly
generated by self-reports.

Hence given consistency, similar concurrent responses triggered by the
same inducers can be used as a marker for authentic SSS. Consistency
test have become golden standart for colour-grapheme synesthesia, where
an inducer is presented (i.e. letters of the alphabet) and the
participant is requested to selected the concurrent colour, using a
colour picker. Individual consistency is then calculated as the distance
between repeated colour responses to the same inducers. Interestingly,
the best colour space to detect colour-grapheme synesthesia is CIE\*LUV,
a colour space developed to be isoform to human perception
[@rothen2013]. Analogously to grapheme-colour synesthetes, consistency
test can be used to diagnose SSS. In that tasks, it is repeatedly asked
to report the position of the inducers on a screen. The total area
between the responses of same inducer (i.e. a triangle if repeated three
times) is then used as characteristic to diagnose SSS. The rationale
being that consistent responses would lead to smaller area than
inconsistent ones [@rothen2016]. This method resembles how number forms
are describe in the single case study [@piazza2006], see Experiment 1.

However disciminating synesthesia using total area has several
limitations. For example high consistency by non-synesthetes can be
achieved by giving all responses on the same screen position (i.e. false
positive). Moreover, this kind of criteria might bias the diagnosis to
include synesthetes with straight lines which leads to less variability
than more complex forms[@Ward].

The goal of the present registered report is to first identify new
features characterizing synesthetes responses based on already available
datasets and test the best working features on a future dataset. The new
features are designed to take advantage of two properties of synesthetic
responses that have not been included in precedent consistency tests.
First, sequentiality on top of single inducer responses the ordered
position between subsequent induces is important. For example the
relative position of August and the other months. From numerical
cognition, ordinality has been acknowledged to be an important semantic
property of numbers, also given their sequential acquisition (i.e. 1 is
learned before 2). Second, thee particular synthetic forms of the
sequential spatial location. These forms might have geometrical
properties. For example months of the year might be represented
circularly (as already described by [@galton1880] for numbers).

To take advantage of sequential and geometrical synesthetic forms, we
harnessed a geo-spatial package[@pebesma2018] to extract geometrical
features from participant x and y coordinate responses. This packages
allows for example to build string or polygons for each repetition and
compare different geometrical features. Those individual geometrical
features are then compared using Receiver Operator Characteristics (ROC)
between individuals grouped as synesthets and control. In the present
*phase I*,we compare ROC on three merged derivation datasets using the
same task on SSS [@rothen2016, @ward; @vanpetersen2020a]. In future
*phase II*, we compare whether the features selected to diagnose SSS in
*phase I,* on a validation dataset that is not yet acquired (registered
report on the open science foundation: <https://osf.io/9efjb/>**).**

# General Methods

*Phase I: present analyses*. We merge three available dataset and
compared available diagnostic criteria across datasets using Receiver
Operator Characteristics (ROC) for different approaches. A first
approach of diagnostic criteria based on a stimulus level consistency.
The rationale being that synesthetes should produce more consistent
(i.e. less variable) across repetitions. Such criteria include area
between repetitions[^1], standard deviation of responses [@ward] or
permuting the responses to compare each responses to a permuted chance
level Root [@root2021]. The second suggested approach is to look at the
geometrical form generated by the responses across repetitions. The
rationale here is that synesthetic responses should have geometrical
feature that differ from controls. For example, several SSS
representations for months are circular. For the form

[^1]: Of which we replicate the original results [@rothen2016] in a
    separate document

Third, we compare whether the features lead to similar ROC
characteristics across the different sets (i.e. for months, weeks and
numbers). Fourth, we compute new candidate geometrical features that
could be used to diagnose SS. Finally we summarize and compare all ROC
and select the best features that class synesthetes from control with
the merged dataset.

*Phase II: future analyses.* On a future dataset to be collected using
the same task, we will compare the predictive power of the selected
features using ROC.

## Materials

A the exception of [@rothen2016] (see
<https://osf.io/6hq94/files/osfstorage>), the data from [@ward;
@vanpetersen2020a] were collected online. The 29 inducers were: the 12
months of a year, 7 days of the week and 10 numbers (i.e. hindo-arabic
numerals from 0 to 9). [@vanpetersen2020a] Also presented 50 and 100
numerals, which we excluded here. [@ward] data is collected using the
Syntoolkit.

## Procedure

The details for each procedure is described in each respective article
[@rothen2016; @ward; @vanpetersen2020a], here we describe the common
task.

Each participant is presented with one one inducer at a time at the
center of a otherwise white screen. The participant is instructed to
click at the screen position that they visualize them. Inducers order is
randomized and each inducer is repeated three times.

*The order of the stimuli was randomized, but such that no stimulus was
repeated until the previous batch of unique stimuli (N = 29) had been
presented.*

Importantly, while the three datasets inclucded in *phase I* include
three repetitions per stimuli, the *phase II* will use four repetition
per stimuli. Hence the cut-offs will be designed to be scalable to
different number of repetitions and stimulus.

# *Phase I* Methods

## *Phase I*: Merge and prepare data

We merged three datasets: @rothen2016\],[@ward] (from:
[https://osf.io/p5xsd/files/osfstorage](#0){.uri}) and
[@vanpetersen2020]. To match the other datasets, stimuli form
[@vanpetersen2020] are translated from dutch to English and for the
stimuli, only numbers from 0 to 9 are kept (excluding 50 and 100). The x
and y coordinates were then separately normalized (z-score) per
participant.

```{r Load data}
#| warning: false
# In the following I upload and merge the data from Ward, Rothen and Van Peters. Data is stored into a full dataset ds (i.e. 1 row per trial) and a dataset per participant ds_Quest (i.e. 1 row per participant).

###  Ward Data
ds_syn       <- read_excel("raw_synaesthetes_consistency_anon.xlsx")
ds_syn$group <- "Syn"
ds_ctl       <- read_excel("raw_controls_consistency_anon.xlsx")
ds_ctl$group <- "Ctl"

ds_Q_syn       <- read_excel("raw_synaesthetes_questionnaire_anon.xlsx")
ds_Q_syn$group <- "Syn"
ds_Q_ctl       <- read_excel("raw_controls_questionnaire_anon.xlsx")
ds_Q_ctl$group <- "Ctl"

# Merge wards datafiles:
ds <- merge(ds_syn,ds_ctl, all = TRUE)
ds_Q <- merge(ds_Q_syn,ds_Q_ctl, all = TRUE)

# Ward only uses those who completed the Questionnaire (i.e. N = 215+252 = 467)
ds <- ds %>% 
  filter(session_id %in% unique(ds_Q$session_id))
 
### Rothen Data

ds_Rothen <- read.csv("~/Documents/SpaceSequenceSynDiagnostic/SpaceSequenceSynDiagnostic/rawdata.txt", sep="")

### Rename variables to match datasets

sum(ds_Q$consistency_score != ds_Q$consistency) # Duplicate variable
ds_Q$consistency <- NULL
ds_Q$...36 <- NULL
ds_Q$...37 <- NULL
ds_Q$mean_simulation_Z <- NULL
ds_Q$SD_simulation <- NULL
ds_Q$`z-score`  <- NULL

rm(ds_syn,ds_ctl,ds_Q_syn,ds_Q_ctl)

ds$ID <- ds$session_id
ds_Q$ID <- ds_Q$session_id

ds_Q$dataSource <- "Ward"
ds$dataSource <- "Ward"

# From Rothen:
names(ds_Rothen)[names(ds_Rothen) == "Group"] <- "group"
ds_Rothen$group <- as.factor(ds_Rothen$group)
levels(ds_Rothen$group) <- c("Ctl","Syn")
names(ds_Rothen)[names(ds_Rothen) == "Inducer"] <- "stimulus"
names(ds_Rothen)[names(ds_Rothen) == "X"] <- "x"
names(ds_Rothen)[names(ds_Rothen) == "Y"] <- "y"
ds_Rothen$SynQuest <- ds_Rothen$group == "Syn"

ds_Rothen$dataSource <- "Rothen"

# From the paper (all the same since lab based):
ds_Rothen$width  <- 1024
ds_Rothen$height <- 768
```

```{r, Merge1 Rothen and Ward data}
## 0.2 Merge data:
# remove non matching colnames: 

ColNames_ds <- colnames(ds_Rothen)[colnames(ds_Rothen) %in% colnames(ds)]

ds <- ds %>%
  select(all_of(ColNames_ds))
ds_Rothen <- ds_Rothen %>%
  select(all_of(ColNames_ds))

# Data:
ds   <- merge(ds,ds_Rothen, all = TRUE)

# Because there is no questionnaire in Nicola's data:
ds$SynQuest[ds$dataSource == "Rothen"] = "NaN"

# Questionnaire:
ID <- unique((ds_Rothen$ID))
ds_Q_Rothen <- as.data.frame(ID)
ds_Q_Rothen$dataSource <- "Rothen"
ds_Q_Rothen <- merge(ds_Q_Rothen, ds_Rothen %>% group_by(ID) %>% select(ID, dataSource, group) %>% filter(row_number() == 1), by = c("ID","dataSource"))

# Append rows:
ds_Q$ID <- as.character(ds_Q$ID)
ds_Q <-  bind_rows(ds_Q,ds_Q_Rothen)

# Clear up:
rm(ds_Q_Rothen, ds_Rothen)

## 0.3 Wrangle dataset

# Add Condition, i.e. stim type:
ds$Cond <- NaN
ds$Cond[ds$stimulus %in% c("1","2","3","4","5","6","7","8","9","0")] <- "number"
ds$Cond[ds$stimulus %in% c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday")] <- "weekday"
ds$Cond[ds$stimulus %in% c("January", "February", "March", "April", "May","June","July","August","September","October","November","December")] <- "month"
```

```{r, Merge2}
### ADD Van Petersen Cortex data

filedir <- "~/Documents/SpaceSequenceSynDiagnostic/VanPetersen/Cortex/di.dcc.DSC_2018.00019_653/Consistency test/Preprocessed data/"
fn <- paste0(filedir,dir(filedir))

ds_PeterCor <- read_excel(fn[1],
                                col_types = c("text", "numeric", "text", 
                                              "text", "numeric", "numeric", "numeric", 
                                              "numeric", "numeric", "numeric", 
                                              "numeric", "numeric", "text", "text", 
                                              "text", "numeric", "numeric", "numeric"))

for(i in 2:length(fn)){
  ds_i <- read_excel(fn[i],
                     col_types = c("text", "numeric", "text", 
                                   "text", "numeric", "numeric", "numeric", 
                                   "numeric", "numeric", "numeric", 
                                   "numeric", "numeric", "text", "text", 
                                   "text", "numeric", "numeric", "numeric"))
    ds_PeterCor <- merge(ds_PeterCor, ds_i, all = TRUE)
}

# Here information about group:
ds_PeterCor_Q <- read_excel("~/Documents/SpaceSequenceSynDiagnostic/VanPetersen/Cortex/di.dcc.DSC_2018.00019_653/Consistency test/Final data files/Consistency_scores.xlsx")

names(ds_PeterCor_Q)[names(ds_PeterCor_Q) == "PPcode"] <- "ID"
names(ds_PeterCor)[names(ds_PeterCor) == "Code"] <- "ID"

# Exclude and Match ID's:
# IDExcl <- unique(ds_PeterCor$ID[is.na(ds_PeterCor$MouseClick.RESPCursorX)])
# ds_PeterCor <- ds_PeterCor %>%
#     filter(!ID %in% IDExcl)

ID_IN <- intersect(ds_PeterCor$ID, ds_PeterCor_Q$ID)
ds_PeterCor <- ds_PeterCor %>%
    filter(ID %in% ID_IN)
ds_PeterCor_Q <- ds_PeterCor_Q %>%
    filter(ID %in% ID_IN)

# Merge data with group and all dataset: 
ds_PeterCor <- ds_PeterCor_Q %>%
  left_join(ds_PeterCor, by = "ID")

# Rename to match:
names(ds_PeterCor)[names(ds_PeterCor) == "Group"] <- "group"
ds_PeterCor[ds_PeterCor$group == "SSS",]$group <- "Syn"
ds_PeterCor[ds_PeterCor$group == "control",]$group <- "Ctl"
names(ds_PeterCor)[names(ds_PeterCor) == "MouseClick.RESPCursorY"] <- "y"
names(ds_PeterCor)[names(ds_PeterCor) == "MouseClick.RESPCursorX"] <- "x"
names(ds_PeterCor)[names(ds_PeterCor) == "word"] <- "stimulus" # Will need to translate to english!
names(ds_PeterCor)[names(ds_PeterCor) == "BlockList.Cycle"] <- "repetition"

ds_PeterCor$width  <- 1920 # "screen with display resolution set to 1920 x 1080, controlled by a Dell computer running Windows 7."
ds_PeterCor$height <- 1080
ds_PeterCor$dataSource <- "PeterCor"

ds_PeterCor <- ds_PeterCor %>%
  select(all_of(ColNames_ds))

# Translate, weekdays:
ds_PeterCor$stimulus[ds_PeterCor$stimulus == "maandag"]   <- "Monday"
ds_PeterCor$stimulus[ds_PeterCor$stimulus == "dinsdag"]   <- "Tuesday"
ds_PeterCor$stimulus[ds_PeterCor$stimulus == "woensdag"]  <- "Wednesday"
ds_PeterCor$stimulus[ds_PeterCor$stimulus == "donderdag"] <- "Thursday"
ds_PeterCor$stimulus[ds_PeterCor$stimulus == "vrijdag"]   <- "Friday"
ds_PeterCor$stimulus[ds_PeterCor$stimulus == "zaterdag"]  <- "Saturday"
ds_PeterCor$stimulus[ds_PeterCor$stimulus == "zondag"]    <- "Sunday"

# Translate, Monthc:
ds_PeterCor$stimulus[ds_PeterCor$stimulus == "januari"]   <- "January"
ds_PeterCor$stimulus[ds_PeterCor$stimulus == "februari"]  <- "February"
ds_PeterCor$stimulus[ds_PeterCor$stimulus == "maart"]     <- "March"
ds_PeterCor$stimulus[ds_PeterCor$stimulus == "april"]     <- "April"
ds_PeterCor$stimulus[ds_PeterCor$stimulus == "mei"]       <- "May"
ds_PeterCor$stimulus[ds_PeterCor$stimulus == "juni"]      <- "June"
ds_PeterCor$stimulus[ds_PeterCor$stimulus == "juli"]      <- "July"
ds_PeterCor$stimulus[ds_PeterCor$stimulus == "augustus"]  <- "August"
ds_PeterCor$stimulus[ds_PeterCor$stimulus == "september"] <- "September"
ds_PeterCor$stimulus[ds_PeterCor$stimulus == "oktober"]   <- "October"
ds_PeterCor$stimulus[ds_PeterCor$stimulus == "november"]  <- "November"
ds_PeterCor$stimulus[ds_PeterCor$stimulus == "december"] <- "December"

# They added 2 numbers that are not in the other datasets:
ds_PeterCor <- ds_PeterCor %>%
    filter(stimulus != "50") %>%
    filter(stimulus != "100")

##### MERGE ####
ds   <- merge(ds,ds_PeterCor, all = TRUE)

# Questionnaire:
names(ds_PeterCor_Q)[names(ds_PeterCor_Q) == "Group"] <- "group"
ds_PeterCor_Q[ds_PeterCor_Q$group == "SSS",]$group <- "Syn"
ds_PeterCor_Q[ds_PeterCor_Q$group == "control",]$group <- "Ctl"

ds_PeterCor_Q$dataSource <- "PeterCor"

ds_PeterCor_Q <- ds_PeterCor_Q %>%
  select(ID, group, `Consistency All`, `Consistency Days`, `Consistency Months`,`Consistency Numbers`, dataSource)

# Append rows:
ds_Q <-  bind_rows(ds_Q,ds_PeterCor_Q)

# Tidy workspace:
rm(ds_PeterCor_Q,ds_PeterCor,ds_i, ID, fn,ColNames_ds,i,ID_IN)
```

```{r Wrangle data}
# Now we can enrich our dataset and process  several checks
# Add Condition, i.e. stim type:
ds$Cond <- NaN
ds$Cond[ds$stimulus %in% c("1","2","3","4","5","6","7","8","9","0")] <- "number"
ds$Cond[ds$stimulus %in% c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday")] <- "weekday"
ds$Cond[ds$stimulus %in% c("January", "February", "March", "April", "May","June","July","August","September","October","November","December")] <- "month"

# Add stimulus repetition number
ds <- ds %>%
  group_by(ID, stimulus) %>%
  arrange(ID, stimulus, .by_group = TRUE) %>%
  mutate(repetition = row_number()) %>%
  ungroup()

# Factorialize group variable to avoid confusions:
ds$group   <- as.factor(ds$group)
ds_Q$group <- as.factor(ds_Q$group)
```

```{r, FilterTrials}
# This might be a critical part for the results. Need to decide:
# (1) how to treat missing data (i.e. NA & NaN)
#   i.e. exclude or keep
# (2) how to treat same coordinate responses
#   per Condition | repetitions or only Condition
#   Will affect z-score and area consistency.
# Those decision have 3 major consequences:
#   sf does not accept missing data. --> but since on the form level, we can exclude those responses and compare it to the area consistency that operates on the stimulus level
#   z score might fail (i.e. if all same coordinates). -->  turn same coordinates to NaN
#   polygons need at least 4 coordinate pairs. --> Do not exclude to many trials
# In general I choose keeping the maximum possible data.

# Important. To reproduce Rothen, the X = -1 and Y = -1 need to be interpreted as missing data (NaN):
ds$x[ds$x == -1] <- NaN
ds$y[ds$y == -1] <- NaN

# Detect ID has the same coordinates across Conditions:
ds <- ds %>%
    group_by(ID, Cond) %>% 
    mutate(SameCoord_x = length(unique(x[!is.na(x)])) == 1,SameCoord_y = length(unique(y[!is.na(y)])) == 1)
# Not group_by(ID, Cond, repetition), because we loose to much data, especially for polygons

# ds %>% filter(ID %in% "36540") %>% filter(Cond %in% "number") %>% select(y, SameCoord_y)

# NaNification:
ds$x[ds$SameCoord_x] <- NaN
ds$y[ds$SameCoord_y] <- NaN
ds$x[is.na(ds$x)]    <- NaN  # In VanPeters, some NA data.
ds$y[is.na(ds$y)]    <- NaN 

```


```{r, FilterTrialsSpread}
# The general concept is to compute the distance between each coordinates with the general per ID centroid.
# From that we can compute the spread (another measure of variance), which is the average distance of each coordinate pairs to the centroid.
# It is an attempt to replicate how synr validate the data: https://datalowe.github.io/synr/articles/validate-data.html
# Centroid between repetitions
ds <- ds %>% 
  group_by(ID, Cond, stimulus) %>%
  mutate(x_Mean = mean(x, na.rm = TRUE), y_Mean = mean(y, na.rm = TRUE))

# Distance from centroid of each coordinates:
ds <- ds %>% 
    group_by(ID, Cond, stimulus) %>%
    group_by(stimulus) %>%
    mutate(dist_x = abs(x - x_Mean),dist_y = abs(y - y_Mean))

# Example:
# Each bar represents the sum of the distances between the coordinates and their centroid per stimuli.
# Note: NaN will lead to empty (no bar). More distant = more variable responses.
# ID_here <- sample(unique(ds$ID),1)
# ds %>% 
#     filter(ID %in% ID_here) %>%
#     group_by(stimulus) %>%
#     summarize(Mdist_x = mean(dist_x, na.rm = TRUE), Mdist_y = mean(dist_y, na.rm = TRUE)) %>%
#     ggplot(aes(y = stimulus, x = Mdist_x + Mdist_y)) +
#     stat_summary(fun = "sum", geom = "bar")

# A very simple approach might be to:
# See ? synr::total_within_cluster_variance
#     Calculate where the ‘middle’ (‘center of gravity’) of all points is located. (in the interactive graph below, the ‘middle’ is marked with a black point)
ds <- ds %>% 
  group_by(ID) %>%
  mutate(x_MidID = mean(x, na.rm = TRUE), y_MidID = mean(y, na.rm = TRUE))
#     For each measured color point, calculate its distance from the ‘middle’.
ds <- ds %>% 
  group_by(ID) %>%
  rowwise() %>%
  mutate(x_MidDist = abs(x_MidID - x),y_MidDist =  abs(y_MidID - y ))
#     Add up all these distances and divide by the total number of color points.
ds <- ds %>% 
  ungroup() %>%
  group_by(ID) %>%
  mutate(spread_x = sum(x_MidDist, na.rm =TRUE)/length(x),spread_y = sum(y_MidDist, na.rm =TRUE)/length(x)) 

# tmp <- ds %>%
#   ungroup() %>%
#   group_by(ID,Cond) %>%
#   summarize(nNaN_x = sum(is.nan(x))/length(x)*100,nNaN_y = sum(is.nan(y))/length(y)*100,ntrials = length(x)) %>% 
#   filter(nNaN_y != 0 & nNaN_x != 0) %>%
#   pivot_wider(names_from = Cond, values_from = c(nNaN_x,nNaN_y))
```


```{r, FilterTrialsOverlap}
# ID_same_x <- ds %>% 
#   filter(SameCoord_x) %>%
#   group_by(ID) %>%
#   filter(row_number() == 1) %>%
#   select(ID)
# 
# ID_same_y <- ds %>% 
#   filter(SameCoord_y) %>%
#   group_by(ID) %>%
#   filter(row_number() == 1) %>%
#   select(ID)

# Same_x_same_y <- ID_same_y$ID[ID_same_y$ID %in% ID_same_x$ID]
# 
# ds$isSame_xy   <- ds$ID %in% Same_x_same_y
# ds_Q$isSame_xy <- ds_Q$ID %in% Same_x_same_y

# n_trials1 = length(ds$x)
# 
# #
# ds <- ds[! ds$SameCoord_x,]
# ds <- ds[! ds$SameCoord_y,] 
# 
# n_trials2 = length(ds$x)
# 
# 
# ds <- ds[! is.nan(ds$x),]
# ds <- ds[! is.nan(ds$y),]
# 
# ds <- ds[! is.na(ds$x),]
# ds <- ds[! is.na(ds$y),]
# 
# n_trials3 = length(ds$x)


# Honestly, this one should not be in the data:
# ds %>% filter(ID == "12_CH")%>% select(Cond,x,y)  %>% print(n = 87) 
# Only responses for months.

# We excluded `r n_trials1 - n_trials2` trials of participants with the same coordinate responses across condition and repetitions.
```



```{r Filter ID}
# Remove ID's with the same coordinate response across experiment
# Those are definitely uninformative.
# "37643" "37928"

# tmp <- ds %>%
#   group_by(ID) %>% # Or  group_by(ID, Cond)
#   filter(length(unique(x[!is.na(x)])) == 1) %>%
#   filter(length(unique(x[!is.na(y)])) == 1)
# 
# ds   <- ds %>% filter(! ID  %in% unique(tmp$ID))
# ds_Q <- ds_Q %>% filter(! ID  %in% unique(tmp$ID))

# If to few data
# na.rm = function(x) {x[!is.na(x)]}
# ds <- ds %>% 
#     group_by(ID,Cond,stimulus) %>% 
#     mutate(Nrep = length(na.rm(x)))
# 
# ds <- ds %>%
#   filter(Nrep == 3)

# Exlude if less than 50% per condition | repetition:
ds <- ds %>% 
    group_by(ID,Cond,repetition) %>% 
    mutate(PercNaN_x = sum(is.nan(x))/length(x))

ds <- ds %>% 
    group_by(ID,Cond,repetition) %>% 
    mutate(PercNaN_y = sum(is.nan(y))/length(y))

ID_toMany_x <- unique(ds$ID[ds$PercNaN_x >= 0.50])
ID_toMany_y <- unique(ds$ID[ds$PercNaN_y >= 0.50])
# Remove if more than 50 %
ds   <- ds %>%    filter(! ID %in% ID_toMany_x)
ds_Q <- ds_Q %>% filter(! ID  %in% ID_toMany_x)

ds   <- ds %>%    filter(! ID %in% ID_toMany_y)
ds_Q <- ds_Q %>% filter(! ID  %in% ID_toMany_y)

# "10_AS"     "2002_PaKr" "PP15" 
```

Invalid responses were additinally set as NaN if a participan'ts coordinate were the same across conditions and repetitions.
Then, we excluded `r length(unique(c(ID_toMany_x,ID_toMany_y)))` participants who had less than 50 % of valid responses per conditions and repetitions. We also manually adjusted the screen size from some participants since those values changed across the experiment. 

```{r Add_zs}
# The main difficulty is that z-score need variability. Now if ID clicked on the same coordinates, no zs can be calculated. So what should it be? 0.

## 0.4 Standardize/scale coordinates
ds <- ds %>%
  group_by(ID) %>% # Not by Cond, so to avoid NaN's
  mutate(x_zs = scale(x)) %>%
  mutate(y_zs = scale(y))

sum(is.nan(ds$x_zs))
sum(is.nan(ds$y_zs))
# ATTENTION WHEN INTERPRETING THIS:
# ds$x_zs[is.na(ds$x_zs)] <- 0
# ds$y_zs[is.na(ds$y_zs)] <- 0
```

```{r ManualAdjust}

# Manually adjust pixels:
# Note: 29 since 29 stimuli
ds$height[ds$ID == 29324] <- 1080
ds$width[ds$ID == 32190 ] <- 1440

# This ID has unexpected changing screen settings, consider exclusion:
ds$width[ds$ID == 33168 ]  <- 308
ds$height[ds$ID == 33168 ] <- 149

ds$width[ds$ID == 35556 ]  <- 1439
ds$height[ds$ID == 35556 ] <- 734

ds$width[ds$ID == 48114 ]  <- 1593
ds$height[ds$ID == 48114 ] <- 671

ds$width[ds$ID == 59854]  <- 1366
ds$height[ds$ID == 59854] <- 663

ds$width[ds$ID == 63127]  <- 1920
ds$height[ds$ID == 63127] <- 880

# Sanity Check: should be empty:
# ID_heigthKO <- ds %>%
#   group_by(ID) %>%
#   mutate(nhig = length(unique(height))) %>%
#   filter(nhig != 1) %>%
#   filter(row_number()==1) %>%
#   pull(ID)
# 
# ID_widthKO <- ds %>%
#   group_by(ID) %>%
#   mutate(nwid = length(unique(width))) %>%
#   filter(nwid != 1) %>%
#   filter(row_number()==1) %>%
#   pull(ID)

ds$Screen_area <- ds$width*ds$height

rm(tmp)
```

```{r TEST}
# Same 84 ID on x and y:
# unique(ds[ds$SameCoord_x,]$ID)
# unique(ds[ds$SameCoord_y,]$ID)

# 2 ID how clicked with the same coordinates across the whole experiment (i.e. impossible to compute SD!)
```

## *Phase I.* Population

```{r PopulationTable}
#| eval: false
#| warning: false
#| include: false
knitr::kable(ds_Q %>%
    group_by(group, dataSource) %>%
    summarize(N = length(ID)) %>%
    pivot_wider(names_from = group, values_from = N))
```

For all the three datasets, the synesthetes were self-reported.

| Source             |     | Synesthetes |           |     | Controls |           | Total |
|---------|---------|---------|---------|---------|---------|---------|---------|
|                    | N   | Age         | n females | N   | Age      | n females |       |
| [@rothen2016]      | 33  | 23.1        | 24        | 37  | 28.2     | 27        | 70    |
| [@vanpetersen2020] | 23  | 23.22       | 20        | 21  | 21.57    | 19        | 44    |
| [@wardb]           | 252 | 37.21       | 202       | 215 | 19.90    | 178       | 467   |
|                    |     |             |           |     |          |           |       |
| Merged             | 308 |             |           | 273 |          |           | 557   |

## *Phase I*. Analysis

First, we replicate consistency methods found in the literature using
the same task ([@rothen2016; @ward; @vanpetersen2020a; @root2021]) and
compare the results.

Second, we extract features based on the form. (C) We harness a
geography package to compute segment based features (D) We compute
polygon based features. (E) Convex Hull (F) Angles.

# *Phase I.* Results Reproduce

# Replicated features

## Standard deviation

```{r, sd}
# Rescale x & y coordinates depending on screen size:
ds$xSc <- ds$x/ds$width
ds$ySc <- ds$y/ds$height

# Compute the SD across all trials (per ID):
ds <- ds %>% 
  ungroup() %>%
  group_by(ID) %>%
  mutate(SD_x = sd(xSc, na.rm = TRUE)) %>%
  mutate(SD_y = sd(ySc, na.rm = TRUE)) 

# Add to ds_Q:
ds_Q <- merge(ds_Q, ds %>% 
                ungroup() %>% group_by(ID) %>% arrange(ID) %>% 
                filter(row_number() == 1) %>% select(ID,SD_x,SD_y),by = "ID"
              )

feature_direction <- c(rep("moreSyn",2))
```

## Triangle area Consistency

**Definition**: Calculating consistency. Each stimulus is represented by
three xy coordinates - (x1, y1), (x2, y2), (x3, y3) - from the three
repetitions. For each stimulus, the area of the triangle bounded by the
coordinates is calculated as follows:\
$Area = (x1y2 + x2y3 + x3y1 – x1y3 – x2y1 – x3y2) / 2$

```{r, triangleArea_fun}

# Define area calculation function
triangle_area <- function(x, y) {
  if(length(x) != 3 | length(y) != 3) return(NA)
  area <- abs(
    x[1]*y[2] + x[2]*y[3] + x[3]*y[1] -
    x[1]*y[3] - x[2]*y[1] - x[3]*y[2]
  ) / 2
  return(area)
}

```

```{r, areaConsistency}
## Compute triangle area by group:
ds <- ds %>%  
  group_by(ID, stimulus) %>%
  mutate(triangle_area = triangle_area(x, y)) %>%
  ungroup()

## Summarize By ID:
ds_ID <- ds %>%
  ungroup() %>% group_by(ID,group) %>%
  summarize(rep_area = mean(triangle_area, na.rm = TRUE))  %>%
  select(ID,rep_area)

# tmp_perID <- ds  %>% ungroup() %>% group_by(ID) %>% filter(row_number() == 1) %>% select(ID, Consistency)
# names(tmp_perID) <- c("ID", "Consistency2")
ds_Q <- merge(ds_Q,ds_ID,by = "ID")
rm(tmp_perID)
feature_direction <- c(feature_direction,"moreCtl")
```

```{r, area_zsConsitency}

ds <- ds %>%  
  group_by(ID, stimulus) %>%
  mutate(rep_area_zs = triangle_area(x_zs, y_zs)) %>%
  ungroup()

ds <- ds %>%  
   group_by(ID,repetition) %>%
   mutate(rep_area_GAzs = (mean(rep_area_zs, na.rm = TRUE)))

tmp_perID2 <- ds %>% ungroup() %>% group_by(ID) %>% filter(row_number() == 1) %>% select(ID, rep_area_GAzs)

ds_Q <- merge(ds_Q,tmp_perID2,by = "ID")
rm(tmp_perID2)

feature_direction <- c(feature_direction,"moreCtl")
```

## Perimeter ($pixel$):

```{r}
### Perimeter function: 
triangle_perimeter <- function(x, y) {
  if(length(x) != 3 | length(y) != 3) return(NA)
  # Side lengths
  a <- sqrt((x[2] - x[1])^2 + (y[2] - y[1])^2)
  b <- sqrt((x[3] - x[2])^2 + (y[3] - y[2])^2)
  c <- sqrt((x[1] - x[3])^2 + (y[1] - y[3])^2)
  perimeter <- a + b + c
  return(perimeter)
}
```

```{r}
## Compute triangle perimeter by group:
ds <- ds %>%  
  group_by(ID, stimulus) %>%
  mutate(triangle_perim_zs = triangle_perimeter(x_zs, y_zs)) %>%
  ungroup()

## Summarize By ID:
tmp_ID <- ds %>%
  ungroup() %>% group_by(ID,group) %>%
  summarize(triangle_perim_GA_zs = mean(triangle_perim_zs)) %>%
  select(ID, triangle_perim_GA_zs)
 
ds_Q <- merge(ds_Q,tmp_ID,by = "ID")
feature_direction <- c(feature_direction,"moreCtl")
```


## Maximum length ($pixel$):

```{r}
### maximum lenth function: 
triangle_maxLen <- function(x, y) {
  if(length(x) != 3 | length(y) != 3) return(NA)
  # Side lengths
  a <- sqrt((x[2] - x[1])^2 + (y[2] - y[1])^2)
  b <- sqrt((x[3] - x[2])^2 + (y[3] - y[2])^2)
  c <- sqrt((x[1] - x[3])^2 + (y[1] - y[3])^2)
  maxLen <- max(c(a, b, c), na.rm = TRUE)
  if(maxLen == -Inf) return(NA)
  return(maxLen)
}
```

```{r}
## Compute max length by group:
ds <- ds %>%  
  group_by(ID, stimulus) %>%
  mutate(triangle_maxLen = triangle_maxLen(x, y)) %>%
  ungroup()

## Summarize By ID:
tmp_ID <- ds %>%
  ungroup() %>% 
  group_by(ID) %>%
  summarize(triangle_maxLen_GA = mean(triangle_maxLen))

ds_Q <- merge(ds_Q,tmp_ID,by = "ID")
feature_direction <- c(feature_direction,"moreCtl")
```

# *Phase I.* Results: Novel features

## Spread 
```{r}
ds_Q <- right_join(ds_Q, 
           (ds %>% 
                ungroup() %>% 
                group_by(ID) %>% 
                filter(row_number() == 1) %>%
                select(ID, spread_y)), by = "ID")

ds_Q <- right_join(ds_Q, 
           (ds %>% 
                ungroup() %>% 
                group_by(ID) %>% 
                filter(row_number() == 1) %>%
                select(ID, spread_x)), by = "ID")

feature_direction <- c(feature_direction,rep("moreSyn",2))
```


## Segment self-intersection

We calculated the number of self intersection for each segments
sperately for repetitions and conditions. For each participant we then
compute the average of self intersections per conditions and
repetitions.

```{r, selfInter_fun}

# Define function: this was generated by chatgpt and it does the correct job after checking

count_self_intersections <- function(x, y, verbose = TRUE) {
  n <- length(x)
  if (n < 4) {
    if (verbose) cat("Need at least 4 points to check for self-intersection.\n")
    return(0)
  }

  # Orientation function
  orientation <- function(p, q, r) {
    val <- (q[2] - p[2]) * (r[1] - q[1]) - (q[1] - p[1]) * (r[2] - q[2])
    if (is.na(val)) return(NA)
    if (val == 0) return(0)
    if (val > 0) return(1) else return(2)
  }

  # Check if q lies on segment pr
  on_segment <- function(p, q, r) {
    if (any(is.na(c(p, q, r)))) return(FALSE)
    q[1] <= max(p[1], r[1]) && q[1] >= min(p[1], r[1]) &&
      q[2] <= max(p[2], r[2]) && q[2] >= min(p[2], r[2])
  }

  # Main intersection check
  segments_intersect <- function(p1, p2, p3, p4) {
    o1 <- orientation(p1, p2, p3)
    o2 <- orientation(p1, p2, p4)
    o3 <- orientation(p3, p4, p1)
    o4 <- orientation(p3, p4, p2)

    if (any(is.na(c(o1, o2, o3, o4)))) return(FALSE)

    # General case
    if (o1 != o2 && o3 != o4) return(TRUE)

    # Special colinear cases
    if (o1 == 0 && on_segment(p1, p3, p2)) return(TRUE)
    if (o2 == 0 && on_segment(p1, p4, p2)) return(TRUE)
    if (o3 == 0 && on_segment(p3, p1, p4)) return(TRUE)
    if (o4 == 0 && on_segment(p3, p2, p4)) return(TRUE)

    return(FALSE)
  }

  count <- 0
  for (i in 1:(n - 2)) {
    for (j in (i + 2):(n - 1)) {
      if (j == i + 1) next  # skip adjacent segments

      p1 <- c(x[i], y[i])
      p2 <- c(x[i + 1], y[i + 1])
      p3 <- c(x[j], y[j])
      p4 <- c(x[j + 1], y[j + 1])

      if (segments_intersect(p1, p2, p3, p4)) {
        count <- count + 1
        if (verbose) {
          cat(sprintf("Intersection #%d: segments (%d-%d) and (%d-%d)\n", count, i, i+1, j, j+1))
        }
      }
    }
  }

  if (verbose) cat("Total crossings:", count, "\n")
  return(count)
}

```

```{r, Self_Intersections}
# Number of intersections for each ID X Cond X repetition
# Important! The dataset must be correctly informed about stimulus order.
ds <- ds %>% 
  group_by(stimulus) %>%
  arrange(stimulus) %>%
  arrange(ordered(stimulus, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday","Saturday","Sunday"))) %>% 
  arrange(ordered(stimulus, levels = c("January", "February", "March", "April", "May","June","July","August","September","October","November","December"))) %>%
  ungroup() %>%
  group_by(ID, Cond,repetition) %>%
  mutate(nLineCross = (count_self_intersections(x,y, verbose = FALSE))) %>%
  arrange(ID)# could also be x_zs and y_zs
```

```{r, selfIntertodsQ}
# Average self-intersections per ID by Cond X repetition
tmp <- ds %>%
  group_by(ID,Cond,repetition) %>%
  filter(row_number() == 1) %>% # keep only 1 row per IDXCondXrep
  ungroup() %>%
  group_by(ID) %>%
  mutate(SelfInter_persegm = mean(nLineCross)) %>%
  filter(row_number() == 1) %>% # keep only 1 row per ID
  select(ID, SelfInter_persegm) 


ds <- merge(ds, tmp,by = "ID")


ds_Q <- right_join(ds_Q, 
           (ds %>% 
                ungroup() %>% 
                group_by(ID) %>% 
                filter(row_number() == 1) %>%
                select(ID, SelfInter_persegm)), by = "ID")


feature_direction <- c(feature_direction,"moreCtl")
```

## Segments (with sf)

We will take advantage of the `sf` package and connect the x and y
coordinates of ordered inducer with a segment. Sf hates NaN's. Either
convert them to 0 (as originally) or remove them. I'll start converting
to 0.

```{r, ds_segm}

# Turn off spherical geometry:
sf::sf_use_s2(FALSE)

# Explicitly enforce item order (i.e. ordinality):

ds_segm <- ds %>%
  group_by(ID, stimulus) %>%
  arrange(stimulus) %>%
  arrange(ordered(stimulus, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday","Saturday","Sunday"))) %>% 
  arrange(ordered(stimulus, levels = c("January", "February", "March", "April", "May","June","July","August","September","October","November","December"))) %>%
  ungroup() %>%
  arrange(ID) %>%
  filter(!is.nan(x_zs), !is.nan(y_zs)) %>% # sf hates NaN! Not needed anymore, NaN's are managed above in the code.
  mutate(
    group = as.character(group),
    ID     = as.character(ID),
    Cond      = as.character(Cond),
    repetition     = as.integer(repetition),
    dataSource = as.character(dataSource)
  ) %>%
  # arrange(ID, Cond, repetition,group) %>%
  group_by(ID, Cond, repetition,group) %>%
  summarise(
    geometry = st_sfc(st_linestring(as.matrix(cbind(x_zs, y_zs)))), # preserves order
    .groups = "drop"
  ) %>%
  st_as_sf(crs = NA)
```

### Segment length

```{r segm length}
ds_segm$leng <- st_length(ds_segm)

ds_segm <- ds_segm %>%
  group_by(ID) %>%
  mutate(segm_leng_GA = mean(leng, na.rm = TRUE))

ds_Q <- right_join(ds_Q, 
                ds_segm %>% 
                ungroup() %>% 
                group_by(ID) %>% 
                filter(row_number() == 1) %>% 
                st_drop_geometry() %>%
                select(ID, segm_leng_GA),by = "ID")


feature_direction <- c(feature_direction,"moreCtl")
```

## Polygon based geometries

### Polygon area

```{r, poly area}
ds_poly          <- st_cast(ds_segm, "POLYGON")
ds_poly$area     <- st_area(ds_poly)

ds_poly <- ds_poly %>%
  group_by(ID) %>%
  mutate(areaPoly_GA = mean(area, na.rm = TRUE))

ds_Q <- right_join(ds_Q, 
                   ds_poly %>% st_drop_geometry() %>% ungroup() %>% group_by(ID) %>% filter(row_number() == 1) %>% select(ID, areaPoly_GA),
                   by = "ID")

feature_direction <- c(feature_direction,"moreSyn")
```

### Polygon simplicity

```{r, polyisSimple}
# Might depend on cast:
ds_poly          <- st_cast(ds_segm, "POLYGON", group_or_split = TRUE)

ds_poly$isSimple <- st_is_simple(ds_poly)

ds_poly <- ds_poly %>%
  group_by(ID) %>%
  mutate(isSimple_GA = mean(isSimple, na.rm = TRUE))

ds_Q <- right_join(ds_Q, 
                   ds_poly %>% st_drop_geometry() %>% ungroup() %>% group_by(ID) %>% filter(row_number() == 1) %>% select(ID, isSimple_GA),
                   by = "ID")

feature_direction <- c(feature_direction,"moreSyn")
```

### Topological validity Structure

From the package description: *"For projected geometries, st_make_valid
uses the lwgeom_makevalid method also used by the PostGIS command
ST_makevalid if the GEOS version linked to is smaller than 3.8.0, and
otherwise the version shipped in GEOS; for geometries having ellipsoidal
coordinates s2::s2_rebuild is being used."* From
<https://postgis.net/docs/ST_IsValid.html>: *value is well-formed and
valid in 2D according to the OGC rules.* (Open Geopsatial Consotrtium)

```{r, topoValid}
ds_poly$isValidStruct <- st_is_valid(ds_poly, geos_method = "valid_structure")

# ds_poly <- ds_poly %>%
#   group_by(ID) %>%
#   mutate(Sum_isValidStruct = sum(isValidStruct, na.rm = TRUE))
# 
# ds_Q <- right_join(ds_Q, 
#                    ds_poly %>% st_drop_geometry() %>% ungroup() %>% group_by(ID) %>% filter(row_number() == 1) %>% select(ID, Sum_isValidStruct),
#                    by = "ID")

ds_poly <- ds_poly %>%
  group_by(ID) %>%
  mutate(isValidStruct_GA = mean(isValidStruct, na.rm = TRUE))

ds_Q <- right_join(ds_Q, 
                   ds_poly %>% st_drop_geometry() %>% ungroup() %>% group_by(ID) %>% filter(row_number() == 1) %>% select(ID, isValidStruct_GA),
                   by = "ID")

feature_direction <- c(feature_direction,"moreSyn")
```

# Permutations

Until now the form based features are computed by chronologically
ordered repetitions. For example, Monday is repeated three times per ID.
The coordinates for Monday presented the first time will always be used
to form the segment/polygon with the Tuesday presented the first time.
However, for consistency, this should be independent from chronological
order. To circumvent this, we can permute the repetitions per
conditions. I predict the permuted averages of the same features should
give rise to better AUC. Since these are time consuming I will only
apply permutations on the criteria that are at stake:\
topological validity.

```{r permuteAcrossRep}
#| eval: false
#| include: false

# Takes about 40 minutes on my machine. So pre-saved the output. 
set.seed(42)


ds <- ds %>%
  group_by(stimulus) %>%
  arrange(stimulus) %>%
  arrange(ordered(stimulus, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday","Saturday","Sunday"))) %>%
  arrange(ordered(stimulus, levels = c("January", "February", "March", "April", "May","June","July","August","September","October","November","December"))) %>%
  ungroup() %>%
  group_by(ID, group, Cond, repetition) %>%
  mutate(StimOrder = 1:length(stimulus))


# First define output matrix:

n_perms <- 100
perm_names <- paste0("n_perm", sprintf("%03d", 1:n_perms))  # X001 … X100

ds_perm <-  as.data.frame(
  matrix(NA_real_, nrow = length(unique(ds$ID)), ncol = n_perms,
         dimnames = list(unique(ds$ID), perm_names))
)

ID_list <- unique(ds$ID)
total = length(ID_list)*n_perms
pb <- txtProgressBar(min = 0, max = total, style = 3)
k <- 0


for(Perm_n in 1:n_perms){
  perm_here <- perm_names[Perm_n]
  for(ID_n in 1:length(unique(ds$ID))){
    
     
    ################ Extract data per ID: ################
    ds_ID <- ds %>%
      filter(ID %in% ID_list[ID_n]) %>%
      select(ID, group, stimulus, Cond, nLineCross, repetition, StimOrder,x_zs,y_zs,SelfInter_persegm)
    
     ################ Apply permutation (sample) across repetitions################
     ds_ID <- ds_ID %>%
      group_by(stimulus) %>%
      mutate(repetition_perm = sample(repetition)) 
    
    ################ Pass to sf ################
    ds_ID_segm <- ds_ID %>%
      group_by(ID, stimulus) %>%
      arrange(stimulus) %>%
      arrange(ordered(stimulus, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday","Saturday","Sunday"))) %>% 
      arrange(ordered(stimulus, levels = c("January", "February", "March", "April", "May","June","July","August","September","October","November","December"))) %>%
      filter(!is.nan(x_zs), !is.nan(y_zs)) %>% # sf hates NaN! Not needed anymore, NaN's are managed above in the code.
      ungroup() %>%
      arrange(ID) %>%
      mutate(
        group = as.character(group),
        ID     = as.character(ID),
        Cond      = as.character(Cond),
        repetition_perm     = as.integer(repetition_perm)
      ) %>%
      group_by(ID, Cond, repetition_perm,group) %>%
      summarise(
        geometry = st_sfc(st_linestring(as.matrix(cbind(x_zs, y_zs)))), # preserves order
        .groups = "drop"
      ) %>%
      st_as_sf(crs = NA) 
    
    ################ Convert to poly and compute validity ################
    ds_ID_poly               <- st_cast(ds_ID_segm, "POLYGON")
    ds_ID_poly$isValidStructPerm <- st_is_valid(ds_ID_poly, geos_method = "valid_structure")
    
    ds_ID_poly <- ds_ID_poly  %>%
      mutate(isValidStructPerm = mean(isValidStructPerm, na.rm = TRUE)) %>%
      filter(row_number() == 1)
    
    ################ save the perumted results ################
        if(!rownames(ds_perm[ID_n,]) == ID_list[ID_n]){
      warning("ID names do not match")
    }
    
    ds_perm[ID_n,Perm_n] <- ds_ID_poly$isValidStructPerm
    k <- k + 1
    setTxtProgressBar(pb, k)
  }
}

# Now average the permutations: 

ds_perm$ID <-rownames(ds_perm)
ds_perm$group <- ds_perm$ID  %in% ds_Q$ID[ds_Q$group == "Ctl"] # is true if Ctl
ds_perm$group[ds_perm$group]  <- "Ctl"
ds_perm$group[ds_perm$group == "FALSE"] <- "Syn"

write.csv2(ds_perm, "permuted_isValid.csv")

```

```{r, AveragePermute}
ds_perm <- read.csv2("permuted_isValid.csv")


## ATTENTION; RECOMPUTE THE PERMUTATION WITH THE ACTUAL DATASET then remove this
ds_perm <- ds_perm %>% filter(ID %in% ds_Q$ID)
ds_Q   <- ds_Q %>% filter(ID %in% ds_perm$ID)
# read.csv2("permuted_isValid.csv")
ds_Perm_perID <-  ds_perm %>% 
  rowwise() %>% 
  mutate(isValid_M_perm_ID = mean(c_across(n_perm001:n_perm100)), isValid_Med_perm_ID = median(c_across(n_perm001:n_perm100))) %>% 
  select (ID, group, isValid_M_perm_ID, isValid_Med_perm_ID)


ds_Q <- right_join(ds_Q, 
                   ds_Perm_perID %>% st_drop_geometry() %>% ungroup() %>% group_by(ID) %>% arrange(ID) %>% filter(row_number() == 1) %>% select(ID, isValid_M_perm_ID, isValid_Med_perm_ID),
                   by = "ID")
feature_direction <- c(feature_direction,rep("moreSyn",2))
```

# Combine valid and area criteria

What if we combined consistency and permuted topological validity?

```{r}
Model3 <- glm(group ~  rep_area_GAzs +isValid_M_perm_ID, data = ds_Q, family = binomial)
pred3 = predict(Model3)
ds_Q$GLMValid_area_zs <- pred3

feature_direction <- c(feature_direction,rep("moreSyn",1))
```

# Combine SD_x and area criteria

Like in Ward

```{r}

Model4 <- glm(group ~ SD_x + rep_area_GAzs , data = ds_Q, family = binomial)
pred4 = predict(Model4)
ds_Q$GLMSD_area_zs <- pred4

feature_direction <- c(feature_direction,"moreSyn")
```

# Test


```{r}
#| eval: false
#| include: false

# First: create wide form: one matrix per (id, rep)
configs_by_rep <- ds %>%
  group_by(ID, group, repetition, stimulus) %>%
  summarise(x = mean(x), y = mean(y), .groups = "drop") %>%
  group_by(ID, group, repetition) %>%
  nest() %>%
  ungroup()

compute_procrustes_stability <- function(dat_list) {
  # dat_list: list of tibbles: each has columns item, x, y
  # Ensure items in same order across reps
  # Turn them into matrices
  
  mats <- lapply(dat_list, function(d) {
    d2 <- d %>% arrange(stimulus)
    as.matrix(d2[, c("x", "y")])
  })
  
  # Need at least 2 reps:
  if (length(mats) < 2) return(NA_real_)
  
  # all pairwise Procrustes distances
  combs <- combn(seq_along(mats), 2)
  dists <- apply(combs, 2, function(idx) {
    ref <- mats[[idx[1]]]
    target <- mats[[idx[2]]]
    # make sure same number of rows:
    n <- min(nrow(ref), nrow(target))
    ref <- ref[1:n, , drop = FALSE]
    target <- target[1:n, , drop = FALSE]
    
    pro <- vegan::procrustes(ref, target, scale = TRUE)
    pro$ss # sum of squares residual; smaller = more similar
  })
  
  mean(dists)
}

procrustes_features <- configs_by_rep %>%
  group_by(ID, group) %>%
  summarise(
    procrustes_stability = compute_procrustes_stability(data),
    .groups = "drop"
  )

```

## kmeans
```{r kmeans}

na.rm = function(x) {x[!is.na(x)]}

cluster_features <- ds %>%
  group_by(ID, Cond) %>%
  summarise(
    k1_tot_within = kmeans(c(na.rm(x_zs),na.rm(y_zs)), centers = 1, nstart = 10)$tot.withinss,
    k2_tot_within = kmeans(c(na.rm(x_zs),na.rm(y_zs)), centers = 2, nstart = 10)$tot.withinss,
    k3_tot_within = kmeans(c(na.rm(x_zs),na.rm(y_zs)), centers = 3, nstart = 10)$tot.withinss,
    k2_gain = (k1_tot_within - k2_tot_within) / k1_tot_within,
    k3_gain = (k1_tot_within - k3_tot_within) / k1_tot_within,
    .groups = "drop"
  )

cluster_features <- cluster_features %>%
    group_by(ID) %>%
    summarise(across(, ~ mean(.x, na.rm = TRUE))
    ) %>% select(-Cond)

ds_Q <- right_join(ds_Q, 
                   cluster_features %>%  ungroup() %>% arrange(ID), by = "ID")
feature_direction <- c(feature_direction,rep("moreCtl",5))

```

## local jumps

```{r}
local_jump_features <- ds %>%
  group_by(ID, Cond) %>%
  summarise(
    step_dist = sqrt(diff(x)^2 + diff(y)^2),
    .groups = "keep"
  ) %>%
  group_by(ID, Cond) %>%
  summarise(
    mean_step = mean(step_dist),
    sd_step   = sd(step_dist),
    n_jumps   = sum( (step_dist - mean_step) / sd_step > 2 ),
    .groups = "drop"
  )

local_jump_features <- local_jump_features %>%
    group_by(ID) %>%
    summarise(across(, ~ mean(.x, na.rm = TRUE))
    ) %>% select(-Cond)

ds_Q <- right_join(ds_Q, 
                   local_jump_features %>%  ungroup() %>% arrange(ID), by = "ID")
feature_direction <- c(feature_direction,rep("moreSyn",3))

```

## correlations
```{r}

Correl <- ds %>%
  group_by(ID, Cond) %>%
  summarise(
    corr = cor(x,y)
  )

tmp <- Correl %>% 
  group_by(ID) %>%
  summarise(M_corr = mean(corr, na.rm = TRUE)) 

ds_Q <- right_join(ds_Q, 
                   tmp %>%  ungroup() %>% arrange(ID), by = "ID")

feature_direction <- c(feature_direction,rep("moreSyn",1))

#### Correlate between repetitions


correls <- ds %>%
  select(x,y,ID,Cond,stimulus,repetition) %>%
  group_by(ID, Cond) %>%
  pivot_wider(
    names_from = repetition,
    values_from = c(x, y),
    names_prefix = "rep"
  ) %>%
  summarise(
    # correlations for x
    r_x_1_2 = cor(x_rep1, x_rep2, use = "pairwise.complete.obs"),
    r_x_1_3 = cor(x_rep1, x_rep3, use = "pairwise.complete.obs"),
    r_x_2_3 = cor(x_rep2, x_rep3, use = "pairwise.complete.obs"),
    
    # correlations for y (remove if you don't need them)
    r_y_1_2 = cor(y_rep1, y_rep2, use = "pairwise.complete.obs"),
    r_y_1_3 = cor(y_rep1, y_rep3, use = "pairwise.complete.obs"),
    r_y_2_3 = cor(y_rep2, y_rep3, use = "pairwise.complete.obs"),
    
    .groups = "drop"
  )

correls <- correls %>%
  group_by(ID) %>%
  summarise(r_x_1_2 = mean(r_x_1_2), r_x_1_3 = mean(r_x_1_3), r_x_2_3 = mean(r_x_2_3),r_y_1_2 = mean(r_y_1_2),r_y_1_3 = mean(r_y_1_3),r_y_2_3 = mean(r_y_2_3),GA_corr = mean(c(r_x_1_2,r_x_1_3,r_x_2_3,r_y_1_2,r_y_1_3,r_y_2_3), na.rm = TRUE) )

ds_Q <- right_join(ds_Q, 
                   correls %>%  ungroup() %>% arrange(ID), by = "ID")

feature_direction <- c(feature_direction,rep("moreSyn",7))
```

```{r}
ds %>%
  group_by(ID, Cond, stimulus) %>%
  mutate(Mean_stim_x = mean(x_zs),Mean_stim_y = mean(y_zs),SD_stim_x = sd(x_zs),SD_stim_y = sd(y_zs))
```


# Compare all features:

## Compute all ROC

```{r, InitializeROC_fun}
## Define function to compute ROC:
Comp_ROC <- function(data, group_col, feature, ID, Ord){
  # Ord: must be: moreSyn or moreCtl. Wheter feature value is moer in Syn or ctl.
  if(!setequal(levels(data[[group_col]]), c("Syn","Ctl"))){
    return(warning("group name must inculde: Syn Ctl"))
    break
  }
  
  if (Ord == "moreSyn") {
    Dirhere <- "<"
  } else if (Ord == "moreCtl") {
    Dirhere <- ">"
  } else {
    return(warning("Ord must be moreSyn or moreCtl"))  
  }
  
  ################ ROC analyses ################ 
  
  ROC_here <- pROC::roc(data[[group_col]] ~ data[[feature]], data, 
                  direction= Dirhere,
                  percent=TRUE,
                  # arguments for ci
                  ci=TRUE, boot.n=100, ci.alpha=0.9, stratified=FALSE,
                  # arguments for plot
                  plot=FALSE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
                  print.auc=TRUE, show.thres=TRUE, print.thres = "best", print.thres.best.method="youden")
  
  # Best threshold using Youden's J
  best_coords <- pROC::coords(ROC_here, "best", 
                              ret = c("accuracy","threshold", "sensitivity","specificity","ppv","npv"), 
                              best.method = "youden")
  

  auc_val <- as.numeric(pROC::auc(ROC_here))
  ci_auc  <- ci.auc(ROC_here)
  
  new_row <- data.frame(
    Feature   = feature,
    AUC        = round(auc_val, 4),
    threshold  = as.numeric(best_coords[["threshold"]]),
    sensitivity= as.numeric(best_coords[["sensitivity"]]),
    specificity= as.numeric(best_coords[["specificity"]]),
    ppv        = as.numeric(best_coords[["ppv"]]),
    npv        = as.numeric(best_coords[["npv"]]),
    ci_low     = as.numeric(ci_auc[1]),
    ci_high    = as.numeric(ci_auc[3]),
    stringsAsFactors = FALSE
  )

  ################ Contingency table ################ 
  
  if (Ord == "moreSyn") {
    data$diagnosis <- ifelse(data[[feature]] >= best_coords$threshold,  "Test : Syn", "Test : Ctl")
  } else if (Ord == "moreCtl") {
    data$diagnosis <- ifelse(data[[feature]] >= best_coords$threshold,  "Test : Ctl","Test : Syn")
  } else {
    warning("Ord must be moreSyn or moreCtl")  
  }
  
  tab_counts <- table(data[[group_col]], data$diagnosis)
  
  tab_percent <- prop.table(tab_counts, margin = 1) * 100
  
  result <- matrix(
    paste0(tab_counts, " (", round(tab_percent, 1), "%)"),
    nrow = nrow(tab_counts),
    dimnames = dimnames(tab_counts)
  )
  ################ Mosaic plot of the contigency table #############
  
  # gp1 <- data %>%
  #   # mutate(test_out = SD_ID_ysc > best_coords[["threshold"]]) %>%
  #   ggplot() +
  #   geom_mosaic(aes(x = product(diagnosis, [[group_col]]), fill = diagnosis)) +
  #   scale_fill_manual(values = c("#66C2A5","#FC8D62")) +
  #   geom_mosaic_text(aes(x = product(diagnosis, [[group_col]]), label = after_stat(.wt))) +
  #   theme_minimal()
  
  ################ General description ################ 
  # Attention, na removed  
  Descr_table <- data %>%
    group_by(!!sym(group_col)) %>%
    summarize(n = length(unique(!!sym(ID))), Mean = mean(!!sym(feature), na.rm = TRUE), SD = sd(!!sym(feature), na.rm = TRUE))

  ################ Return outputs ################ 
  return(list(ROC_properties = new_row, Coningency_table =result, Descr_table = Descr_table, ROC_curve = ROC_here))

}
```

```{r compROCallFeat}
#| message: false
#| warning: false
all_cols <- colnames(ds_Q)
feature_list      <- all_cols[42:length(all_cols)]

# Visual check if correct matches:
# rbind(feature_list,feature_direction)

# Now we will loop through features:
# INITIALIZE ROC_curves
ROC_curves <- list()
ROC_contingency <- list()

# Initialize dataframe to collect relevant ROC values:
All_ROC <- data.frame(Feature=character(),
                      AUC = character(),
                 threshold=integer(),
                 sensitivity=integer(),
                 specificity=integer(),
                 ppv = integer(),
                 npv = integer(),
                 high_ci = integer(),
                 low_ci = integer(),
                 stringsAsFactors=FALSE)

# Loop by features:
for(fit_n in 1:length(feature_list)){
  ROC_here <- Comp_ROC(ds_Q, "group", feature_list[fit_n],"ID",feature_direction[fit_n])
  
  ROC_curves      <- c(ROC_curves,list(ROC_here$ROC_curve,ROC_here$ROC_properties$Feature))
  All_ROC         <- rbind(All_ROC, ROC_here$ROC_properties)
  ROC_contingency <- c(ROC_contingency,list(ROC_here$Coningency_table,ROC_here$ROC_properties$Feature))
}

```

# Summaries

## Summary table:

```{r, summaryROC}
library(kableExtra)

All_ROC_round <- All_ROC %>% 
    mutate_if(is.numeric, round,2)
All_ROC_round <- All_ROC_round[order(All_ROC_round$AUC, decreasing = TRUE), ]


# Add colours for a nice table:
n_color <- length(All_ROC_round$Feature)

All_ROC_round[2] <- lapply(All_ROC_round[2], function(x) {
    cell_spec(x, bold = T, 
              background  = spec_color(x, end = 0.9,  palette = paletteer::paletteer_c("ggthemes::Red-Green Diverging", n_color), direction = -1 ),
    )
})

All_ROC_round[4] <- lapply(All_ROC_round[4], function(x) {
    cell_spec(x, bold = T, 
              background  = spec_color(x, end = 0.9,  palette = paletteer::paletteer_c("ggthemes::Red-Green Diverging", n_color), direction = -1 ),
    )
})

All_ROC_round[5] <- lapply(All_ROC_round[5], function(x) {
    cell_spec(x, bold = T, 
              background = spec_color(x, end = 0.9,  palette = paletteer::paletteer_c("ggthemes::Red-Green Diverging", n_color), direction = -1 ),
    )
})

kbl(All_ROC_round, escape = F, align = "c") %>%
  kable_classic("striped", full_width = F)
```

## Summary plot

```{r, summaryplot}

nFeat <- length(ROC_curves)
Odd_idx  <- seq(1,nFeat,by=2)
Pair_idx <- seq(2,nFeat,by=2)

ROC_listed <- list()
Feattype <- c()
for(i in 1:nFeat){
  ROC_listed[[i]] <- ROC_curves[[Odd_idx[i]]]
  Feattype <- c(Feattype,ROC_curves[[Pair_idx[i]]])
}

ggroc(ROC_listed) +
  geom_segment(aes(x = 0, xend = 100, y = 100, yend = 0),
               color="grey", size = 0.01) +
  scale_color_manual(labels = Feattype,values =pals::brewer.pastel1(nFeat)) + # pals::cols25(nFeat)
  theme_minimal()

```

# Summary excluding overlapping responses

We remove participants that have overlapping
responses in one conditions (i.e. the exact same coordinate responses
for all repetitions in one conditions).

```{r ROCnoOverlap}
#| eval: false
#| include: false


ds_Q2 <- ds_Q # %>% filter(!isSame_xy)

# Now we will loop through features:
# INITIALIZE ROC_curves
ROC_curves2 <- list()
ROC_contingency2 <- list()

# Initialize dataframe to collect relevant ROC values:
All_ROC2 <- data.frame(Feature=character(),
                      AUC = character(),
                 threshold=integer(),
                 sensitivity=integer(),
                 specificity=integer(),
                 ppv = integer(),
                 npv = integer(),
                 high_ci = integer(),
                 low_ci = integer(),
                 stringsAsFactors=FALSE)

# Loop by features:
for(fit_n in 1:length(feature_list)){
  ROC_here2 <- Comp_ROC(ds_Q2, "group", feature_list[fit_n],"ID",feature_direction[fit_n])
  
  ROC_curves2      <- c(ROC_curves2,list(ROC_here2$ROC_curve,ROC_here2$ROC_properties$Feature))
  All_ROC2         <- rbind(All_ROC2, ROC_here2$ROC_properties)
  ROC_contingency2 <- c(ROC_contingency,list(ROC_here2$Coningency_table,ROC_here2$ROC_properties$Feature))
}

```

```{r noOverlapTable}
#| eval: false
#| include: false


All_ROC_round2 <- All_ROC2 %>% 
    mutate_if(is.numeric, round,2)
All_ROC_round2 <- All_ROC_round2[order(All_ROC_round2$AUC, decreasing = TRUE), ]


# Add colours for a nice table:
n_color <- length(All_ROC_round2$Feature)

All_ROC_round2[2] <- lapply(All_ROC_round2[2], function(x) {
    cell_spec(x, bold = T, 
              background  = spec_color(x, end = 0.9,  palette = paletteer::paletteer_c("ggthemes::Red-Green Diverging", n_color), direction = -1 ),
    )
})

All_ROC_round2[4] <- lapply(All_ROC_round2[4], function(x) {
    cell_spec(x, bold = T, 
              background  = spec_color(x, end = 0.9,  palette = paletteer::paletteer_c("ggthemes::Red-Green Diverging", n_color), direction = -1 ),
    )
})

All_ROC_round2[5] <- lapply(All_ROC_round2[5], function(x) {
    cell_spec(x, bold = T, 
              background = spec_color(x, end = 0.9,  palette = paletteer::paletteer_c("ggthemes::Red-Green Diverging", n_color), direction = -1 ),
    )
})

kbl(All_ROC_round2, escape = F, align = "c") %>%
  kable_classic("striped", full_width = F)
```

## Summary plot

```{r no OverlapPlot}
#| eval: false
#| include: false

library(RColorBrewer)


nFeat <- length(ROC_curves2)
Odd_idx  <- seq(1,nFeat,by=2)
Pair_idx <- seq(2,nFeat,by=2)

ROC_listed2 <- list()
Feattype <- c()
for(i in 1:nFeat){
  ROC_listed2[[i]] <- ROC_curves2[[Odd_idx[i]]]
  Feattype <- c(Feattype,ROC_curves2[[Pair_idx[i]]])
}

ggroc(ROC_listed2) +
  geom_segment(aes(x = 0, xend = 100, y = 100, yend = 0),
               color="grey", size = 0.01) +
  scale_color_manual(labels = Feattype,values =pals::cols25(nFeat)) + 
  theme_minimal()
```

## Compare Consistency and validity

When removing the overlapping responses both criteria seems to work at
pairs!

```{r}
#| message: false
ROC_Valid <- plot.roc(group ~ isValid_M_perm_ID , ds_Q, percent = TRUE, col="red")
ROC_Cons  <- lines.roc(group ~ rep_area_GAzs , ds_Q, percent = TRUE,col ="green")

roc.test(ROC_Valid, ROC_Cons)
```

## *Phase II* Methods

## *Phase II* Materials:

Materials are described here
<https://osf.io/pjb6e/?view_only=d467ebf4c1f94076ae4ac61298255065>.

## *Phase II* Planned population

[**https://osf.io/6h8dx**](https://osf.io/6h8dx)

# Discussion

From the different features we extracted, topological validity across
the repetitions appeared to be the one leading to the largest Area Under
the Curve. The optimal cutoff was exactly 1.5, leading to a sensitivity
() and specificity ().

The optimal criterion ineeds to be informed about the order between
inducers (i.e. to construct the polygons) and interestingly suggests
that synthetic inducer are structurally mapped following topological
rules analogous to geographical space structures. Hence suggesting a
spatial nature for the synthetic forms of space sequence synesthetes.

Although an optimal tool to discriminate SSS might be particularly
relevant for experimental purposes, it is important to consider some
limitations. These consistency tools are designed with a limited set of
sequential stimuli (i.e. months, weeks and the first ten natural
numbers). <!--# Diversity --> Other sequences might also be represented
in particular spatial positions such as temperature, ect.
<!--# Continuum --> Another point is that rather than categorical,
synesthesia might be present on a continuum in the general population.
In that case diagnostic cutoffs might not be relevant, rather a score
would be necessary. <!--# Cirularity --> Finally, there might also be an
issue of circularity - as with many diagnostics : how synesthesia is
defined determines how synestetes are detected which are the groups on
which synesthesia is defined. <!--# External validity --> This is
particularly relevant when the two diagnostic criteria on which validity
are compared are self-reports (i.e. being conscious) and consistency.

Overlapping responses. A methodological issue concerns participants that
give the same responses across conditions. These responses are a
complication since we can't infer whether those conditions did not give
rise to a synesthetic response in a synthesete or whether it is from a
control that was confused about the instructions. On a methodological
level, those responses can critically bias the diagnostic criteria. On
one side excluding those responses would imbalance the number of
responses by participant, on the other side including these responses
might bias the diagnostic.

See also [@root2025].

# Supplementary: per Conditions

```{r, ROCperCondprep}
All_ROC_Cond <- data.frame(Feature=character(),
                      AUC = character(),
                 threshold=integer(),
                 sensitivity=integer(),
                 specificity=integer(),
                 ppv = integer(),
                 npv = integer(),
                 high_ci = integer(),
                 low_ci = integer(),
                 stringsAsFactors=FALSE)
```

## Per Conditions

```{r, PerCondition}
ds <- ds %>%  
    group_by(ID,repetition,Cond) %>%
    mutate(Consistency_zs_Cond = (sum(rep_area_zs)))

Cond_list <- unique(ds$Cond)
rm(ROC_curvesCond) # we are concatenating list, so make sure they don't already exist

for(Cond_i in 1:3) {
  PerCond <- ds %>%
    filter(Cond %in% Cond_list[Cond_i]) %>%
    Comp_ROC(., "group", "Consistency_zs_Cond","ID","moreCtl")
  
  PerCond$ROC_properties$Feature <- paste0(PerCond$ROC_properties$Feature, "_", Cond_list[Cond_i])
  All_ROC_Cond <- rbind(All_ROC_Cond, PerCond$ROC_properties)
  
  if(Cond_i == 1){
    # INITIALIZE ROC_curves
    ROC_curvesCond <- list(PerCond$ROC_curve,PerCond$ROC_properties$Feature)
  }
  
  ROC_curvesCond <- c(ROC_curvesCond,list(PerCond$ROC_curve,PerCond$ROC_properties$Feature))
}


ds_poly <- ds_poly %>%
  group_by(ID,Cond) %>%
  mutate(GA_isValidStructCond = mean(isValidStruct, na.rm = TRUE))

for(Cond_i in 1:3) {
  PerCond <- ds_poly %>%
    filter(Cond %in% Cond_list[Cond_i]) %>%
    mutate(group = as.factor(group)) %>%
    st_drop_geometry() %>%
    Comp_ROC(., "group", "GA_isValidStructCond","ID","moreCtl")
  PerCond$ROC_properties$Feature <- paste0(PerCond$ROC_properties$Feature, "_", Cond_list[Cond_i])
  All_ROC_Cond <- rbind(All_ROC_Cond, PerCond$ROC_properties)
  ROC_curvesCond <- c(ROC_curvesCond,list(PerCond$ROC_curve,PerCond$ROC_properties$Feature))
}


# Sum per ID
ds <- ds %>%
  group_by(ID,Cond) %>%
  mutate(SelfInter_persegmCond = sum(nLineCross)) 

for(Cond_i in 1:3) {
  PerCond <- ds %>%
    filter(Cond %in% Cond_list[Cond_i]) %>%
    
    Comp_ROC(., "group", "SelfInter_persegmCond","ID","moreCtl")
  PerCond$ROC_properties$Feature <- paste0(PerCond$ROC_properties$Feature, "_", Cond_list[Cond_i])
  All_ROC_Cond <- rbind(All_ROC_Cond, PerCond$ROC_properties)
  ROC_curvesCond <- c(ROC_curvesCond,list(PerCond$ROC_curve,PerCond$ROC_properties$Feature))
}


ggroc(list(ROC_curvesCond[[1]],ROC_curvesCond[[3]],ROC_curvesCond[[5]],ROC_curvesCond[[7]],ROC_curvesCond[[9]],ROC_curvesCond[[11]],ROC_curvesCond[[13]],ROC_curvesCond[[15]],ROC_curvesCond[[17]],ROC_curvesCond[[19]])) +
  scale_color_manual(labels = c(ROC_curvesCond[[2]],ROC_curvesCond[[4]],ROC_curvesCond[[6]],ROC_curvesCond[[8]],ROC_curvesCond[[10]],ROC_curvesCond[[12]],ROC_curvesCond[[14]],ROC_curvesCond[[16]],ROC_curvesCond[[18]],ROC_curvesCond[[20]]), values = brewer.pal(11, name = "Set3")) + 
  theme_minimal()


```


## Summary table

```{r summaryPerCond}
knitr::kable(All_ROC_Cond[order(All_ROC_Cond$AUC, decreasing = TRUE), ], digits = 2)
```

# References
